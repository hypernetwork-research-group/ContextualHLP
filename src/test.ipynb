{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariodemaio/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import IMDBHypergraphDataset, collate_fn, train_test_split, ARXIVHypergraphDataset, COURSERAHypergraphDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import HypergraphConv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import trange, tqdm\n",
    "from torch_geometric.data.hypergraph_data import HyperGraphData\n",
    "from torch_geometric.nn.aggr import MeanAggregation\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, precision_score, roc_curve\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensivity_specificity_cutoff(y_true, y_score):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    idx = np.argmax(tpr - fpr)\n",
    "    return thresholds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_transform(data: HyperGraphData):\n",
    "    data.edge_index = data.edge_index[:, torch.isin(data.edge_index[1], (data.edge_index[1].bincount() > 1).nonzero())]\n",
    "    unique, inverse = data.edge_index[1].unique(return_inverse=True)\n",
    "    data.edge_attr = data.edge_attr[unique]\n",
    "    data.edge_index[1] = inverse\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data: HyperGraphData):\n",
    "    data.x = torch.rand_like(data.x)\n",
    "    data.edge_attr = torch.rand_like(data.edge_attr)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = IMDBHypergraphDataset(\"./data\", pre_transform=pre_transform, transform=transform, force_reload=True)\n",
    "train_dataset, test_dataset, _, _, _, _ = train_test_split(dataset, test_size=0.3)\n",
    "train_dataset, validation_dataset, _, _, _, _ = train_test_split(train_dataset, test_size=0.3)\n",
    "# Train dataset is for train\n",
    "# Test indices for evaluation (the forward pass is performed on the whole network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "writer = SummaryWriter(f\"./logs/{dataset.DATASET_NAME}/{randint(0, 10000)}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset._data.x = torch.randn_like(dataset._data.x)\n",
    "# dataset._data.edge_attr = torch.randn_like(dataset._data.edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=500,\n",
    "    num_workers=8,        # Uncomment for faster loading\n",
    "    pin_memory=True,      # Uncomment for faster loading\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initalize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 hidden_channels: int,\n",
    "                 out_channels: int,\n",
    "                 num_layers: int = 1):\n",
    "        super(Model, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        self.in_norm = nn.LayerNorm(in_channels)\n",
    "        self.in_proj = nn.Linear(in_channels, hidden_channels)\n",
    "        self.e_proj = nn.Linear(in_channels, hidden_channels)\n",
    "        self.e_norm = nn.LayerNorm(in_channels)\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            setattr(self, f\"n_norm_{i}\", nn.LayerNorm(hidden_channels))\n",
    "            setattr(self, f\"e_norm_{i}\", nn.LayerNorm(hidden_channels))\n",
    "            setattr(self, f\"hgconv_{i}\", HypergraphConv(\n",
    "                hidden_channels,\n",
    "                hidden_channels,\n",
    "                use_attention=True,\n",
    "                concat=False,\n",
    "                heads=1\n",
    "            ))\n",
    "            setattr(self, f\"skip_{i}\", nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.aggr = MeanAggregation()\n",
    "        self.linear = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, x_e, edge_index):\n",
    "        x = self.in_norm(x)\n",
    "        x = self.in_proj(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x_e = self.e_norm(x_e)\n",
    "        x_e = self.e_proj(x_e)\n",
    "        x_e = self.activation(x_e)\n",
    "        x_e = self.dropout(x_e)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            n_norm = getattr(self, f\"n_norm_{i}\")\n",
    "            e_norm = getattr(self, f\"e_norm_{i}\")\n",
    "            hgconv = getattr(self, f\"hgconv_{i}\")\n",
    "            skip = getattr(self, f\"skip_{i}\")\n",
    "            x = n_norm(x)\n",
    "            x_e = e_norm(x_e)\n",
    "            x = self.activation(hgconv(x, edge_index, hyperedge_attr=x_e)) + \\\n",
    "                skip(x)\n",
    "\n",
    "        x = self.aggr(x[edge_index[0]], edge_index[1])\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle as pkl\n",
    "from torch_geometric.nn.aggr import MinAggregation\n",
    "from torch_geometric.nn import Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigvecs = eigvecs[:, :-512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = dataset.num_features\n",
    "out_channels = 1\n",
    "\n",
    "model = Model(\n",
    "    in_channels,\n",
    "    hidden_channels,\n",
    "    out_channels,\n",
    "    1,\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "test_criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling(h: HyperGraphData):\n",
    "    edge_index = h.edge_index.clone()\n",
    "    edge_index[0] = torch.randint(0, h.num_nodes, (edge_index[1].shape[0], ), device=h.x.device)\n",
    "    h_edge_attr = torch.vstack((h.edge_attr, h.edge_attr))\n",
    "    h.y = torch.vstack((\n",
    "        torch.ones((h.edge_index[1].max() + 1, 1), device=h.x.device),\n",
    "        torch.zeros((edge_index[1].max() + 1, 1), device=h.x.device),\n",
    "    ))\n",
    "    edge_index[1] += edge_index[1].max() + 1\n",
    "    edge_index = torch.hstack((h.edge_index, edge_index))\n",
    "    h_ = HyperGraphData(\n",
    "        x=h.x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=h_edge_attr,\n",
    "        y=h.y,\n",
    "        num_nodes=h.num_nodes,\n",
    "    )\n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 917/1200 [25:16<08:12,  1.74s/it]Exception in thread Thread-1019 (_pin_memory_loop):\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/usr/lib/python3.13/threading.py\"\u001b[0m, line \u001b[35m1041\u001b[0m, in \u001b[35m_bootstrap_inner\u001b[0m\n",
      "    \u001b[31mself.run\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/dariodemaio/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/ipykernel/ipkernel.py\"\u001b[0m, line \u001b[35m766\u001b[0m, in \u001b[35mrun_closure\u001b[0m\n",
      "    \u001b[31m_threading_Thread_run\u001b[0m\u001b[1;31m(self)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/usr/lib/python3.13/threading.py\"\u001b[0m, line \u001b[35m992\u001b[0m, in \u001b[35mrun\u001b[0m\n",
      "    \u001b[31mself._target\u001b[0m\u001b[1;31m(*self._args, **self._kwargs)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/dariodemaio/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/pin_memory.py\"\u001b[0m, line \u001b[35m61\u001b[0m, in \u001b[35m_pin_memory_loop\u001b[0m\n",
      "    \u001b[31mdo_one_step\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/home/dariodemaio/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/pin_memory.py\"\u001b[0m, line \u001b[35m37\u001b[0m, in \u001b[35mdo_one_step\u001b[0m\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \u001b[35m\"/usr/lib/python3.13/multiprocessing/queues.py\"\u001b[0m, line \u001b[35m120\u001b[0m, in \u001b[35mget\u001b[0m\n",
      "    return \u001b[31m_ForkingPickler.loads\u001b[0m\u001b[1;31m(res)\u001b[0m\n",
      "           \u001b[31m~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/home/dariodemaio/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/torch/multiprocessing/reductions.py\"\u001b[0m, line \u001b[35m541\u001b[0m, in \u001b[35mrebuild_storage_fd\u001b[0m\n",
      "    fd = df.detach()\n",
      "  File \u001b[35m\"/usr/lib/python3.13/multiprocessing/resource_sharer.py\"\u001b[0m, line \u001b[35m57\u001b[0m, in \u001b[35mdetach\u001b[0m\n",
      "    with \u001b[31m_resource_sharer.get_connection\u001b[0m\u001b[1;31m(self._id)\u001b[0m as conn:\n",
      "         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/usr/lib/python3.13/multiprocessing/resource_sharer.py\"\u001b[0m, line \u001b[35m86\u001b[0m, in \u001b[35mget_connection\u001b[0m\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \u001b[35m\"/usr/lib/python3.13/multiprocessing/connection.py\"\u001b[0m, line \u001b[35m519\u001b[0m, in \u001b[35mClient\u001b[0m\n",
      "    c = SocketClient(address)\n",
      "  File \u001b[35m\"/usr/lib/python3.13/multiprocessing/connection.py\"\u001b[0m, line \u001b[35m647\u001b[0m, in \u001b[35mSocketClient\u001b[0m\n",
      "    \u001b[31ms.connect\u001b[0m\u001b[1;31m(address)\u001b[0m\n",
      "    \u001b[31m~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mFileNotFoundError\u001b[0m: \u001b[35m[Errno 2] No such file or directory\u001b[0m\n",
      " 76%|███████▋  | 917/1200 [25:17<07:48,  1.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, h \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader), leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m      8\u001b[39m     h = h.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     h_ = \u001b[43mnegative_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     y_train = model(h_.x, h_.edge_attr, h_.edge_index)\n\u001b[32m     11\u001b[39m     loss = criterion(y_train, h_.y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mnegative_sampling\u001b[39m\u001b[34m(h)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnegative_sampling\u001b[39m(h: HyperGraphData):\n\u001b[32m      2\u001b[39m     edge_index = h.edge_index.clone()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     edge_index[\u001b[32m0\u001b[39m] = torch.randint(\u001b[32m0\u001b[39m, \u001b[43mh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_nodes\u001b[49m, (edge_index[\u001b[32m1\u001b[39m].shape[\u001b[32m0\u001b[39m], ), device=h.x.device)\n\u001b[32m      4\u001b[39m     h_edge_attr = torch.vstack((h.edge_attr, h.edge_attr))\n\u001b[32m      5\u001b[39m     h.y = torch.vstack((\n\u001b[32m      6\u001b[39m         torch.ones((h.edge_index[\u001b[32m1\u001b[39m].max() + \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), device=h.x.device),\n\u001b[32m      7\u001b[39m         torch.zeros((edge_index[\u001b[32m1\u001b[39m].max() + \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), device=h.x.device),\n\u001b[32m      8\u001b[39m     ))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/torch_geometric/data/hypergraph_data.py:86\u001b[39m, in \u001b[36mHyperGraphData.num_nodes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     81\u001b[39m num_nodes = \u001b[38;5;28msuper\u001b[39m().num_nodes\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# For hypergraphs, `edge_index[1]` does not contain node indices.\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Therefore, the below code is used to prevent `num_nodes` being\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# estimated as the number of hyperedges.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.edge_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m num_nodes == \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_edges\u001b[49m):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.edge_index[\u001b[32m0\u001b[39m]) + \u001b[32m1\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m num_nodes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/torch_geometric/data/hypergraph_data.py:77\u001b[39m, in \u001b[36mHyperGraphData.num_edges\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.edge_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m + \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "test_dataset_ = negative_sampling(test_dataset._data).to(device)\n",
    "validation_dataset_ = negative_sampling(validation_dataset._data).to(device)\n",
    "for epoch in trange(1200):\n",
    "    model.train()\n",
    "    # 1. Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    for i, h in tqdm(enumerate(loader), leave=False):\n",
    "        h = h.to(device)\n",
    "        h_ = negative_sampling(h)\n",
    "        y_train = model(h_.x, h_.edge_attr, h_.edge_index)\n",
    "        loss = criterion(y_train, h_.y)\n",
    "        loss.backward()\n",
    "        writer.add_scalar(\"Loss/train\", loss.item(), epoch * len(loader) + i)\n",
    "        cutoff = sensivity_specificity_cutoff(h_.y.cpu().detach().numpy(), y_train.cpu().detach().numpy())\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_validation = model(validation_dataset_.x.to(device), validation_dataset_.edge_attr.to(device), validation_dataset_.edge_index.to(device)) # The test forward is performed on the whole network\n",
    "        y_validation = torch.sigmoid(y_validation)\n",
    "        loss = test_criterion(y_validation, validation_dataset_.y)\n",
    "        cutoff = sensivity_specificity_cutoff(validation_dataset_.y.cpu().numpy(), y_validation.cpu().numpy())\n",
    "        writer.add_scalar(\"Loss/validation\", loss.item(), epoch)\n",
    "        roc_auc = roc_auc_score(validation_dataset_.y.cpu().numpy(), y_validation.cpu().numpy())\n",
    "        writer.add_scalar(\"ROC_AUC/validation\", roc_auc, epoch)\n",
    "        accuracy = accuracy_score(validation_dataset_.y.cpu().numpy(), (y_validation.cpu().numpy() >= cutoff).astype(int))\n",
    "        writer.add_scalar(\"accuracy/validation\", accuracy, epoch)\n",
    "        precision = precision_score(validation_dataset_.y.cpu().numpy(), (y_validation.cpu().numpy() >= cutoff).astype(int), average='micro')\n",
    "        writer.add_scalar(\"precision/validation\", precision, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'entire_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "y_test = model(test_dataset_.x, test_dataset_.edge_attr, test_dataset_.edge_index)\n",
    "cm = confusion_matrix(\n",
    "    test_dataset_.y.cpu().numpy(),\n",
    "    (y_test > cutoff).cpu().numpy(),\n",
    "    labels=[0, 1],\n",
    "    normalize='true'\n",
    ")\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m roc = \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset_\u001b[49m\u001b[43m.\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/sklearn/metrics/_ranking.py:618\u001b[39m, in \u001b[36mroc_auc_score\u001b[39m\u001b[34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m    410\u001b[39m     {\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my_true\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m     labels=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    430\u001b[39m ):\n\u001b[32m    431\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) \\\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[33;03m    from prediction scores.\u001b[39;00m\n\u001b[32m    433\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    615\u001b[39m \u001b[33;03m    array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\u001b[39;00m\n\u001b[32m    616\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m     y_type = \u001b[43mtype_of_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my_true\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m     y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    620\u001b[39m     y_score = check_array(y_score, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/sklearn/utils/multiclass.py:333\u001b[39m, in \u001b[36mtype_of_target\u001b[39m\u001b[34m(y, input_name, raise_unknown)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse_pandas:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33my cannot be class \u001b[39m\u001b[33m'\u001b[39m\u001b[33mSparseSeries\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mSparseArray\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_multilabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmultilabel-indicator\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# DeprecationWarning will be replaced by ValueError, see NEP 34\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# We therefore catch both deprecation (NumPy < 1.24) warning and\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# value error (NumPy >= 1.24).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/sklearn/utils/multiclass.py:172\u001b[39m, in \u001b[36mis_multilabel\u001b[39m\u001b[34m(y)\u001b[39m\n\u001b[32m    170\u001b[39m warnings.simplefilter(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, VisibleDeprecationWarning)\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     y = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_y_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (VisibleDeprecationWarning, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e).startswith(\u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1055\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1053\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1058\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1059\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/sklearn/utils/_array_api.py:839\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    837\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    838\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m839\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/torch/_tensor.py:1225\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype=dtype)\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy().astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "roc = roc_auc_score(test_dataset_.y.cpu().numpy(), y_test.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'train_test_split.<locals>.TestDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest_dataset.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/torch/serialization.py:965\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    964\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Contextual_HLP/.venv/lib/python3.13/site-packages/torch/serialization.py:1211\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1208\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m persistent_id(obj)\n\u001b[32m   1210\u001b[39m pickler = PyTorchPickler(data_buf, protocol=pickle_protocol)\n\u001b[32m-> \u001b[39m\u001b[32m1211\u001b[39m \u001b[43mpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1212\u001b[39m data_value = data_buf.getvalue()\n\u001b[32m   1213\u001b[39m zip_file.write_record(\u001b[33m\"\u001b[39m\u001b[33mdata.pkl\u001b[39m\u001b[33m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get local object 'train_test_split.<locals>.TestDataset'"
     ]
    }
   ],
   "source": [
    "torch.save(test_dataset, 'test_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'train_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(loader, 'loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8284078220887882\n",
      "0.8244632708964201\n",
      "0.8270478599582132\n",
      "0.8270715366340654\n",
      "0.8250100013507919\n",
      "0.8288865338828547\n",
      "0.8280987389655856\n",
      "0.8245213770067454\n",
      "0.8267536998327248\n",
      "0.828201760654126\n",
      "0.8281269491565808\n",
      "0.8271206829973824\n",
      "0.8260570394043789\n",
      "0.8273067419429025\n",
      "0.8265697398154901\n",
      "0.826025685069798\n",
      "0.8274246813181038\n",
      "0.8293747779554814\n",
      "0.8263814194234519\n"
     ]
    }
   ],
   "source": [
    "for i in range(19):\n",
    "    test_dataset_ = negative_sampling(test_dataset)\n",
    "    y_test = model(test_dataset_.x.to(device), test_dataset_.edge_attr.to(device), test_dataset_.edge_index.to(device)) # The test forward is performed on the whole network\n",
    "    y_test = torch.sigmoid(y_test)\n",
    "    roc_auc = roc_auc_score(test_dataset_.y.cpu().numpy(), y_test.detach().cpu().numpy())\n",
    "    print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
